A number of helpful scripts for dealing with H5 files, testing segmentation and
generally playing with `home-robot` + `slap_manipulation` code-base.

## Anatomy of H5 files

H5 files are our chosen representation for storing episodic data generated through
the usage of the (`collect_data.py`)[../projects/data_collection/collect_h5.py]
script. It uses the `EpisodeManager` class to manage starting, ending and cleaning
up between distinct episodes. Underneath (`Recorder` class)[../src/home_robot_hw/home_robot_hw/ros/recorder.py]
is used to interface with the robot's hardware and environment to get and format
data as needed. All scripts assume the following structure:
```python 
# H5-file-|
#         |-episode0-|
#         |          |-head_rgb/<frame_number> (RGB Image)
#         |          |-head_depth/<frame_number> (DEPTH Image)
#         |          |-head_xyz/<frame_number> (XYZ point-cloud)
#         |          |-ee_pose/<frame_number> (end-effector 6D pose)
# ...
``` 

In the above example `episode0` has trial-index 0.

## Introspection Scripts

### Visualize Images and Trajectories in H5 files generated by `Recorder class`

Usage:  
`python visualize_episodes_h5.py --h5-file <path/to/file> --trial <index> [optional] --replay`

The script shows you all images collected as part of trial-number `index` and
also plots end-effector poses onto ROS TFtree which can be viewed through Rviz.
`--replay` flag replays the end-effector trajectory on the robot wherever
it is right now assuming trajectory stores relative motions per segment.

### Test Detic Detections on Observations coming from camera-stream

- Launch bootup script on the robot
- Run `test_detic_on_obs.py` from $HOME_ROBOT_ROOT:
  `python projects/slap_manipulation/scripts/test_detic_on_obs.py --category object1 --category object2`
- If no objects are given the script will use default category vocab
  `MY_CATEGORIES = ["cup", "bottle", "drawer", "basket", "bowl", "computer", "mug"]`

### Read and visualize H5s for detailed data view

Usage:  
`python read_h5s.py --data-dir <path/to/data> --template <template-string>`

Shows input image, name of the task, name of the file, name of the episode, the
labeled `FAIL/SUCCESS` status and names of all keys present for each episode.

## H5 Post-processing Scripts

### Add Detic Semantic Features to H5 Files, Visualize and Debug

Usage:  
`python add_detic_features_to_h5.py --data-dir <path/to/data> --template <glob/template> --mode
[read/write/visualize]`  
Stand-alone script to add Detic features to pre-recorded images
Assumes data is stored in H5s with the following structure:
```python
# H5-file-|
#         |-episode0-|
#         |          |-head_rgb/<frame_number> (RGB Image)
#         |          |-head_depth/<frame_number> (DEPTH Image)
#         |          |-head_xyz/<frame_number> (XYZ point-cloud)
#         |        ++|-head_semantic_frame
#         |        ++|-head_semantic_mask
```
As outlined above two new keys are added to each episode which store the
semantic frame (visualizable BBox output) and mask for the head camera. 
Remember to change `MY_CATEGORIES` and `TASK_TO_OBJECT_MAP` variables within the
script to your specific case.

The script has following 3 helpful modes:
1. Read: Like a dry-run, reads information from H5 and shows the masks Detic generates for given categories.
1. Write: If masks look as expected in "read" mode use this to write them to the H5 file
1. Visualize: This reads the added masks from each episode and saves them to disk. 
Useful for persistence as well as to debug if any files do not have mask.

### Edit Demonstrations Per Episode

Usage:  
`python edit_demonstrations.py --data-dir <folder/with/all/H5s> --template <template/to/glob/for/H5s>`  

If user needs to tweak demonstrations in a consistent manner (e.g. opening the
drawers a few more inches than demonstrated) then this script provides an example
for how to manipulate trajectories stored within. Each episode that is processed
gets a new "edited" key so that the manipulation is not applied multiple times.

### Creating training, testing and validation splits from recorded H5s

Usage: `python create_h5_splits.py -d <path/to/data> -t <template> --train-num X --val-num Y -n
<name of split file>`

We record all episodes in one go without really dividing them up into train,
test and validation files. Thus in order to run a systematic learning routine
on this data we need to do this as a post-processing step. This file reads
the names of all episodes in every H5 file globbed by `<path/to/data/template>`
and generates a YAML with provided number of entries under `train` (X), `test` (total-X-Y) and
`val` (Y) keys.

### Renaming task-names for consistency across episodes

Usage 1:  
`python rename_task_names.py --data-dir <path/to/data> --template <glob template> --config-file`  

Usage 2:  
`python rename_task_names.py --data-dir <path/to/data> --template <glob template> --from-key <key
to change> --to-key <replacement key name>`

A couple of methods to allow user to rename task-name string in all episodes. 
Switch the usage mode by calling the right method in the script.

### Labeling mislabeled demonstrations as failure or success

Usage:  
`python label_demo_status.py --data-dir <path/to/data> --template <glob template> --mode [read/write]`

Script to label episode as success/failure (key: `demo_status`) for cases which were mislabeled by
data-collector. Visualizes the following then asks if user wants to relabel the episode: 
       point-cloud, rgb-image, number-of-keyframes and current demo_status label

Label is written when mode is write, otherwise above information is printed per episode and script exits.

### Labeling interaction points as extra supervision

Usage:  
`python label_interaction_points.py --data-dir <path/to/data> --template <glob template> --mode [read/write]`

Script to label interaction points for SLAP dataset for tasks
needing explicit supervision, like `pour-into-bowl` where gripper does not explicitly touch bowl.

Script supports following modes:  
  1. Read: Shows 0th image of each episode and associated labeled point cloud
  2. Write: Shows 0th image and queries if user wants to label an interaction point

## Evaluation Scripts

### Evaluating `GeneralLanguageAgent`

- `eval_lang_agent.py` is the entry-point for tinkering with `GeneralLanguageAgent` and `GeneralLanguageEnv`
- Recommend starting a run by entering given command from `$HOME_ROBOT_ROOT`: 

 `python projects/slap_manipulation/scripts/eval_lang_agent.py --testing --task-id 4 --dry-run`
- `--dry-run` flag will make sure the environment doesn't actually make the robot move
- `--testing` flag is for running one of the predefined test-tasks rather than fetching plans from oracle
- `--task-id` is just the index of predefined test-task you wish to run
